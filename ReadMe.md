# Distributed-Statistical-Computing-Work

**ğŸ˜¸Author: [WangZiduan](https://github.com/wj19816)**

Big Data course series of Central University of Finance and Economics, lecture notes and assignments of Distributed Computing taught by Li Feng.

This repository records some of my exercises in distributed computing classes.

[toc]

## 10/15: Hadoop Streaming


æœ¬æ¬¡ä½œä¸šæ˜¯è‡ªå·±å°è¯•å†™ä¸€ä¸ªHadoop Streamingä»£ç ã€‚ä¸ºäº†æ›´æ–¹ä¾¿å’Œå¸¸è§„åŒ–åœ°æäº¤ä»»åŠ¡ï¼Œæˆ‘ä»¬é¦–å…ˆå†™ä¸€ä¸ªè‡ªå·±çš„`main.sh`æ–‡ä»¶ã€‚`main.sh`ä¼šåŒ…å«ä¸€äº›æˆ‘ä»¬å¸¸ç”¨çš„æŒ‡ä»¤ï¼Œä¹‹ååªéœ€è¦ä¿®æ”¹éƒ¨åˆ†å‚æ•°ï¼Œå°±å¯ä»¥æäº¤ä¸åŒçš„MapReduceä»»åŠ¡äº†ã€‚

### main.sh

`main.sh`éœ€è¦è¾“å…¥å››ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼š

1. mapperæ–‡ä»¶è·¯å¾„
2. reduceræ–‡ä»¶è·¯å¾„
3. éœ€è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„
4. è¾“å‡ºç»“æœçš„è·¯å¾„

ç„¶ååœ¨å·¥ç¨‹æ–‡ä»¶å¤¹ä¸‹è¿è¡Œ`sh main.sh mapper reducer inputfile outputpath `ä»£ç ï¼Œå³å¯æäº¤MapReduceã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```shell
#!/bin/bash
echo '>>>>>>>>>>>>>>>> start:' `date`   #æ‰“å°å‡ºä»»åŠ¡å¼€å§‹æ—¶é—´
BEGIN_TIME=`date +%s`                   #å°†ä»»åŠ¡å¼€å§‹æ—¶é—´èµ‹å€¼ç»™BEGIN_TIME,ä¸ºäº†åç»­è®¡ç®—ä»»åŠ¡è¿è¡Œæ—¶é—´

set -u                                  #å½“æ‰§è¡Œæ—¶ä½¿ç”¨åˆ°æœªå®šä¹‰è¿‡çš„å˜é‡,åˆ™æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ 

num=$#                        #å¾—åˆ°è¾“å…¥å‚æ•°çš„ä¸ªæ•°
if [ $num -ne 4 ]; then       #åˆ¤æ–­è¾“å…¥å‚æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚,å¦åˆ™è¾“å‡ºæç¤ºä¿¡æ¯
    echo -e "This program is designed for 1 mapper + 1 reducer + 1 inputfile."
    echo -e "Please input mapper file, reducer file name, input file path and result output path.
    echo -e "eg. sh main.sh mapper reducer inputfile outputpath"
    exit 1                    #å¦‚æœå‚æ•°ä¸ªæ•°ä¸æ­£ç¡®,é€€å‡ºä»»åŠ¡
else           
    MAPPER=${1}               #å‚æ•°æ­£ç¡®,å°†å‚æ•°èµ‹ç»™å¯¹åº”çš„å˜é‡
    REDUCER=${2}
    INPUTFILE=${3}
    OUTPUTPATH=${4}
fi

#è®¾å®šä»»åŠ¡çš„æœ¬åœ°ç»å¯¹è·¯å¾„
PWD=$(cd $(dirname $0); pwd) 
cd $PWD 1> /dev/null 2>&1

#è®¾å®šä»»åŠ¡åç§°
TASKNAME=task_wj

#hadoopé…ç½®
HADOOP_HOME=/usr/lib/hadoop-current              #hadoopå®‰è£…è·¯å¾„
HADOOP_PREFIX=/user/devel/path     #hadoopæ–‡ä»¶å­˜æ”¾è·¯å¾„
HADOOP_INPUT_DIR=${HADOOP_PREFIX}/${INPUTFILE}   #è¾“å…¥æ–‡ä»¶åœ¨hadoopä¸Šçš„å­˜æ”¾è·¯å¾„
HADOOP_OUTPUT_DIR=${HADOOP_PREFIX}/output/${OUTPUTPATH}    #è¾“å‡ºæ–‡ä»¶å¤¹åœ¨hadoopä¸Šçš„è·¯å¾„

#è¾“å‡ºhadoopé…ç½®,ä¾¿äºæ£€æŸ¥æ˜¯å¦è®¾å®šé”™è¯¯
echo $HADOOP_HOME
echo $HADOOP_INPUT_DIR
echo $HADOOP_OUTPUT_DIR

#åˆ é™¤å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶å¤¹
hadoop fs -rmr $HADOOP_OUTPUT_DIR 

#æäº¤hadoop streamingä»»åŠ¡
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
    -input ${HADOOP_INPUT_DIR} \
    -output ${HADOOP_OUTPUT_DIR} \
    -file ${MAPPER} ${REDUCER} \
    -mapper ${MAPPER} \
    -reducer ${REDUCER}

#å¦‚æœä»»åŠ¡è¾“å‡ºç»“æœä¸ç­‰äº0,è¯´æ˜ä»»åŠ¡å‡ºé”™,é€€å‡ºä»»åŠ¡
if [ $? -ne 0 ]; then
    echo 'error'
    exit 1
fi

END_TIME=`date +%s`       

#å¦‚æœä»»åŠ¡è¿è¡ŒæˆåŠŸ,è®¡ç®—ä»»åŠ¡è¿è¡Œæ—¶é—´,å¹¶æ‰“å°æˆåŠŸä¿¡æ¯
echo '******Total cost '  $(($END_TIME-$BEGIN_TIME)) ' seconds'
echo '>>>>>>>>>>>>>>>> end:' `date`
echo "=============SUCCESSFUL============="

exit 0
```



###  Eg1: python+bash

> **Todoï¼šè¾“å‡ºæ•°æ®é›†ä¸­ç¬¦åˆè¦æ±‚çš„æŸå‡ åˆ—**
>
> Mapper: mapper.sh
>
> Reducer: reducer.py

è¿è¡Œï¼š`sh main.sh mapper.sh reducer.py titanic.csv output/python+bash`

#### Mapper: mapper.sh

æ‰“å°å‡ºæ•°æ®é›†çš„å…¶ä¸­å››åˆ—ã€‚

```shell
#! /bin/bash

awk -F '\t' '{printf ("%s\t%s\t%s\t%s\n", $1, $2, $4,$5)}'
```

#### Reducer: reducer.py

æ‰“å°å‡º`t.txt`ä¸­æ ‡æ³¨å‡ºçš„è¡Œä¸­çš„ä¸¤åˆ—æ•°æ®ã€‚

```python
#! /usr/bin/python python3

import os
import sys

f = open('t.txt', 'r')
d = {}
for line in f:
    cs = line.strip()
    d[cs] = 0
f.close()


d1 = []
for line in sys.stdin:
    l = line.strip().split('\t')
    if len(l) != 4:
        continue
    [PassengerId, Survived, Sex, Age] = l
    if PassengerId in d and PassengerId not in d1:
        d1.append(PassengerId)
        res = [PassengerId, Survived]
        print('\t'.join(res))
```

#### Result

![image-20201026102632000](0-LearnHadoop/img/python+bash.png)



###  Eg2: print-colums

> **Todoï¼šé€‰æ‹©è¾“å‡ºæ•°æ®é›†ä¸­ç¬¬1ã€2ã€10ã€12åˆ—çš„æ•°æ®**
>
> Mapper: mapper.sh
>
> Reducer: catï¼ˆç³»ç»Ÿè‡ªå¸¦ï¼‰

è¿è¡Œï¼š`sh main.sh mapper.sh /usr/bin/cat titanic.csv output/print-colums`

#### Mapper: mapper.sh

```shell
#! /bin/bash

# print some columns of my data
awk -F ',' '{printf ("%s\t%s\t%s\t%s\n",  $1, $2, $10, $12)}'
```

#### Result

![image-20201026102610833](0-LearnHadoop/img/print-column.png)



###  Eg3: python-stdin

> **Todoï¼šè®¡ç®—è¾“å…¥æ–‡ä»¶çš„è¡Œæ•°**
>
> Mapper: catï¼ˆç³»ç»Ÿè‡ªå¸¦ï¼‰
>
> Reducer: wc.py

è¿è¡Œï¼š`sh main.sh /usr/bin/cat wc.py LICENSE.txt output/python-stdin`

#### Reducer: wc.py

```python
#! /usr/bin/env python3
#ç¬¬ä¸€è¡Œæ˜¯æŒ‡å®špython3è·¯å¾„

import  sys
count = 0
data = []
for line in sys.stdin:
  count += 1
  data.append(line)
print(count) #print goes to sys.stdout  
```

#### Result

![image-20201026102814078](0-LearnHadoop/img/print-stdin.png)



###  Eg4: stocks

> **Todoï¼šè®¡ç®—å„æ”¯è‚¡ç¥¨å„å¤©çš„å¹³å‡æ”¶ç›Š**
>
> Mapper: mapper.py
>
> Reducer: stock_day_avg.R

è¿è¡Œï¼š`sh main.sh mapper.py stock_day_avg.R stocks.txt output/stocks`

#### Mapper: mapper.py

```python
#! /usr/bin/env python3
import sys

for line in sys.stdin:
    part = line.strip()
    print(part)
```

#### Reducer: stock_day_avg.R

```R
#! /usr/bin/env Rscript

options(warn=-1)
sink("/dev/null")

input <- file("stdin", "r")
while(length(currentLine <- readLines(input, n=1, warn=FALSE)) > 0)
{
    fields <- unlist(strsplit(currentLine, ","))
    lowHigh <- c(as.double(fields[3]), as.double(fields[6]))
    stock_mean <- mean(lowHigh)
    #sink()
    cat(fields[1], fields[2], stock_mean, "\n", sep="\t")
    #sink("/dev/null")
}

close(input)
```

#### Result

![image-20201026103106126](0-LearnHadoop/img/stocks.png)



###  Try-My: Titanic survival

æ ¹æ®ä¸Šé¢å¯¹è€å¸ˆæä¾›ä¾‹å­çš„å°è¯•ï¼Œå·²ç»åˆæ­¥æŒæ¡äº†å†™Hadoop Streamingçš„èƒ½åŠ›ï¼Œå› æ­¤å°è¯•ç”¨è‡ªå·±çš„æ•°æ®é›†å†™ä¸€ä¸ªè‡ªå·±çš„ç¨‹åºã€‚

> **Todoï¼šè®¡ç®—titanicä¸€å…±å­˜æ´»äººæ•°ã€ç”·æ€§äººæ•°ã€å¥³æ€§äººæ•°å’Œæ‰€æœ‰ä¹˜å®¢çš„å¹³å‡å¹´é¾„ã€‚**
>
> Mapper: mapper.py
>
> Reducer: reducer.py

è¿è¡Œï¼š`sh main.sh mapper.py reducer.py titanic.csv output/titanic-survival`

#### Mapper: mapper.py

æ‰“å°å‡ºæ•°æ®é›†çš„ç¬¬1ã€3ã€4åˆ—ï¼Œåˆ†åˆ«ä»£è¡¨æ˜¯å¦å­˜æ´»ã€æ€§åˆ«å’Œå¹´é¾„ã€‚

```python
#! /usr/bin/env python3
import sys

i = 1
for line in sys.stdin:
    if i == 1:   #è·³è¿‡æ ‡é¢˜è¡Œ
        i = 0
        continue
    col = line.strip().split(',')
    cols = [col[i] for i in (1,3,4)]  #[Survived,Pclass,Sex,Age]
    line = '\t'.join(cols)
    print(line) 
```

#### Reducer: reducer.py

è®¡ç®—ä¸€å…±å­˜æ´»äººæ•°ã€ç”·æ€§äººæ•°ã€å¥³æ€§äººæ•°å’Œæ‰€æœ‰ä¹˜å®¢çš„å¹³å‡å¹´é¾„ã€‚

```python
#! /usr/bin/env python3

import sys

survive = 0
male = 0
female = 0
age = []

# line_i: [Survived,Sex,Age]
for line in sys.stdin:
    cols = line.strip().split('\t')
    survive += int(cols[0])
    if cols[1] == 'male':
        male += 1
    else:
        female += 1
    age.append(float(cols[2]))

avg_age = sum(age) / len(age)
print("Total Survived:{}\tMale:{}\tFemale:{}\t\t\tAll passengers's average age is {}".format(survive,male,female,round(avg_age,2)))
```

#### Result

![image-20201026102121114](0-LearnHadoop/img/titanic.png)



## 10/22: Data Cleaning

æœ¬æ¬¡ä½œä¸šé’ˆå¯¹Kaggleä¸Šçš„[US Used cars dataset](https://www.kaggle.com/ananaymital/us-used-cars-dataset)è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚æ•°æ®é¢„è§ˆå¦‚ä¸‹ï¼š

![image-20201101113311570](1-DataClean/img/data-preview.png)

åŸå§‹æ•°æ®é›†ä¸­æœ‰**3000,000**æ¡æ ·æœ¬ï¼Œ**66**ä¸ªå˜é‡ã€‚ç”±äºç›®å‰æ•°æ®é›†ä¸­æœ‰è®¸å¤šæ— ç”¨å˜é‡åŠæ— æ³•ä½¿ç”¨çš„å˜é‡ï¼Œä¸ºäº†å¾—åˆ°èƒ½å¤Ÿè¿›è¡Œåç»­åˆ†æçš„æ•°æ®é›†ï¼Œé¦–å…ˆæˆ‘ä»¬å…ˆæŸ¥çœ‹ä¸€ä¸‹æ•°æ®é›†ä¸­å„ä¸ªå˜é‡çš„ç¼ºå¤±æƒ…å†µä¸å–å€¼æƒ…å†µã€‚ï¼ˆè€ƒè™‘åˆ°åŸå§‹æ•°æ®é‡è¾ƒå¤§ä¸”æœåŠ¡å™¨å‹åŠ›è¾ƒå¤§ï¼Œåé¢å…ˆåˆ©ç”¨10000æ¡æ ·æœ¬çš„æ•°æ®é›†è¿›è¡Œè¯´æ˜ã€‚ï¼‰

### 1. ç¼ºå¤±å€¼æƒ…å†µåŠå˜é‡å¯èƒ½å–å€¼æƒ…å†µåˆ†æ

é¦–å…ˆæˆ‘ä»¬å†™ä¸€ä¸ªmapperç¨‹åºï¼Œè®¡ç®—æ•°æ®é›†ä¸­æ‰€æœ‰å˜é‡çš„ç¼ºå¤±å€¼ä¸ªæ•°ï¼Œå¹¶è¾“å‡ºå„ä¸ªå˜é‡çš„å¯èƒ½å–å€¼é›†åˆåŠé›†åˆå†…çš„å…ƒç´ ä¸ªæ•°ã€‚

#### missing.sh

```bash
#!/bin/bash
echo "===================================================================="
echo -e "\033[33m [INFO] è®¡ç®—å„å˜é‡ç¼ºå¤±å€¼ä¸ªæ•°åŠå¯èƒ½å–å€¼æ•° \033[0m "
echo "===================================================================="

echo '>>>>>>>>>>>>>>>> start:' `date`
BEGIN_TIME=`date +%s`

# main dir
PWD=$(cd $(dirname $0); pwd)
cd $PWD 1> /dev/null 2>&1

TASKNAME=task_wj

# hadoop client
HADOOP_HOME=/usr/lib/hadoop-current
HADOOP_PREFIX=/user/devel/path
HADOOP_INPUT_DIR=${HADOOP_PREFIX}/sample_10000.csv
HADOOP_OUTPUT_DIR=${HADOOP_PREFIX}/output/1015-preprocess/missing

echo $HADOOP_HOME
echo $HADOOP_INPUT_DIR
echo $HADOOP_OUTPUT_DIR

hadoop fs -rmr $HADOOP_OUTPUT_DIR #åˆ é™¤å·²æœ‰çš„outputæ–‡ä»¶å¤¹

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
    -input ${HADOOP_INPUT_DIR} \
    -output ${HADOOP_OUTPUT_DIR} \
    -file 'missing.py' 'merge.py' \
    -mapper "python3 missing.py" \
    -reducer "python3 merge.py"

if [ $? -ne 0 ]; then
    echo 'error'
    exit 1
fi

END_TIME=`date +%s`

echo '******Total cost '  $(($END_TIME-$BEGIN_TIME)) ' seconds'
echo '>>>>>>>>>>>>>>>> end:' `date`
echo "=============SUCCESS=============="

echo "===================================================================="
echo -e "\033[33m [INFO] å„å˜é‡ç¼ºå¤±å€¼ä¸ªæ•°åŠå¯èƒ½å–å€¼æ•°ä¸º: \033[0m "
hadoop fs -cat ${HADOOP_OUTPUT_DIR}/*
echo "===================================================================="

exit 0
```

è¿è¡Œ`sh missing.sh`è¿›è¡Œç¬¬ä¸€æ­¥æ•°æ®å¤„ç†ã€‚

#### missing.py

```python
#! /usr/env/python python3

######---------------è®¡ç®—å„å˜é‡ç¼ºå¤±å€¼ä¸ªæ•°,å¯èƒ½å–å€¼æ•°åŠå–å€¼æƒ…å†µ---------------#######

import sys,re,csv
import numpy as np
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='logger.log', level=logging.INFO, format=' %(asctime)s - %(levelname)s - %(message)s')
logging.info('======Start!=====')
logging.info('Get ready to count missing values...')

# è®¾ç½®å˜é‡åˆå§‹å€¼
count = 0
uniques = [[] for i in range(66)]
missing = np.array([0 for i in range(66)])
isnull = [None,'',' ','-','--']

# å¼€å§‹è¯»å–æ•°æ®
lines = csv.reader(sys.stdin)
for line in lines:
    n = len(line)
    if line[0] == 'vin': 
        continue
    
    try: 
        assert n == 66  #åˆ¤æ–­æ­£åˆ™è¡¨è¾¾å¼æ˜¯å¦åŒ¹é…æ­£ç¡®
        count += 1

        uniques = [uniques[i]+[line[i]] if line[i] not in uniques[i] else uniques[i] for i in range(n)]
        miss_ind = [i in isnull for i in line ]
        missing[miss_ind] += 1

    except:
        continue

# ç”Ÿæˆè¾“å‡ºæ•°æ®
var_miss = [str(x) for x in np.insert(missing,0,count)]
var_out = ','.join(var_miss)

uniq_num = [str(len(uniq)) for uniq in uniques]
uniq_num_out = ','.join(uniq_num)

uniq_det = [','.join(uniq) for uniq in uniques]
uniq_out = ';'.join(uniq_det)

# è¾“å‡ºæ ¼å¼: ç¼ºå¤±å€¼æ•° \t å¯èƒ½å–å€¼æ•° \t æ‰€æœ‰å¯èƒ½å–å€¼åˆ—è¡¨(var1;var2;...;varN)
# count,missing1,missing2,...,missingN \t  uniq1,uniq2,...,uniqN \t u1,u2,u3;u1;u2,u3;...
print(var_out,'\t',uniq_num_out,'\t',uniq_out,'\n')
```

è¿è¡Œ`missing.py`ä¼šå¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå¤„ç†ã€‚ç”±äºåˆ©ç”¨Hadoopè®¡ç®—æ—¶æ€»æ•°æ®é›†ä¼šè¢«åˆ†æˆè‹¥å¹²ä¸ªmapï¼Œå› æ­¤å¯èƒ½ä¼šè¾“å‡ºè‹¥å¹²è¡Œæ•°æ®ã€‚å› æ­¤ï¼Œéœ€è¦åˆ©ç”¨`merge.py`å¯¹è¿™äº›æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œå¾—åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ•°æ®åˆ†ææŠ¥å‘Šã€‚

#### merge.py

```python
#! /usr/env/python python3

######---------------åˆå¹¶missingå¤„ç†ç»“æœ---------------#######

import sys,re,csv
import numpy as np
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='logger.log', level=logging.INFO, format=' %(asctime)s - %(levelname)s - %(message)s')
logging.info('æ­£åœ¨åˆå¹¶ç¬¬ä¸€æ­¥é¢„å¤„ç†ç»“æœ...')

missing = []
uniques = [[] for i in range(66)]

for line in sys.stdin:
    line = line.split('\t')
    missing += map(int,line[0].split(','))
    uniq = line[2].split(';')
    uniques = [list(set(uniques[i]+uniq[i].split(','))) for i in range(66)]

var_miss = [i/missing[0] for i in missing[1:]]
uniq_num = [len(uniq) for uniq in uniques]
uniq_det = [','.join(uniq) for uniq in uniques]
uniq_out = ';'.join(uniq_det)

print('====å„å˜é‡ç¼ºå¤±æ¯”ä¾‹:====')
print(var_miss)

print('====å„å˜é‡å–å€¼å¯èƒ½æ•°:====')
print(uniq_num)

print('====å„å˜é‡å–å€¼å¯èƒ½æƒ…å†µ:====')
print(uniq_out)
```



ç»è¿‡ç¬¬ä¸€æ­¥ï¼Œä¼šå¾—åˆ°æ‰€æœ‰å˜é‡çš„è¾“å‡ºæŠ¥å‘Šï¼Œå¦‚ä¸‹ï¼š

<img src="1-DataClean/img/missing-out.png" alt="image-20201101170834415" style="zoom:50%;" />

æ ¹æ®è¿™ä¸ªè¾“å‡ºæŠ¥å‘Šï¼Œç»“åˆä¹‹å‰çš„æ•°æ®é¢„è§ˆï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç¡®å®šå„ä¸ªå˜é‡éœ€è¦è¿›è¡Œçš„å¤„ç†ã€‚



### 2. ç¼ºå¤±å€¼å¤„ç†ã€å˜é‡å‰”é™¤åŠæ•°æ®æ ¼å¼è½¬æ¢

æ ¹æ®ç¬¬ä¸€æ­¥å¾—åˆ°çš„æ•°æ®æŠ¥å‘Šï¼Œåˆ†åˆ«åˆ†æå„ä¸ªå˜é‡çš„æƒ…å†µï¼Œå¾—åˆ°ä¸‹è¡¨ï¼š

<img src="1-DataClean/img/data-detail-1.png" alt="image-20201101221204364" style="zoom:50%;" />

<img src="1-DataClean/img/data-detail-2.png" alt="image-20201101221307296" style="zoom:50%;" />

æ ¹æ®è¡¨ä¸­åˆ†æç»“æœç¼–å†™MapReduceç¨‹åºè¿›è¡Œé¢„å¤„ç†ã€‚å…·ä½“é€»è¾‘å¦‚ä¸‹ï¼š

- åˆ é™¤æ ‡é¢˜è¡Œ
- å¦‚æœæ•°æ®è¯»å–æ˜¯å¦æ­£ç¡®ï¼Œå¦‚æœé”™è¯¯ï¼Œè·³è¿‡è¯¥æ¡æ ·æœ¬
- åˆ é™¤éƒ¨åˆ†æ•°å€¼å‹å˜é‡ä¸­çš„å•ä½ï¼ˆå¯èƒ½ä¸ºinã€seatsã€galã€RPMç­‰ï¼‰
- å°†äºŒåˆ†ç±»å˜é‡`(True,False)`è®°ä¸º`(0ï¼Œ1)`
- å°†éƒ¨åˆ†å­˜åœ¨åŒä¸€ä¸ªå˜é‡ä¸­çš„æ•°å€¼å‹å˜é‡åˆ†ç¦»
- å¯¹åˆ†ç±»å˜é‡äº§ç”Ÿå“‘å˜é‡
- å‰”é™¤ä¸é€‚åˆè¿›è¡Œæ•°æ®åˆ†æçš„å˜é‡

#### clean.sh

```bash
#!/bin/bash
echo "===================================================================="
echo -e "\033[33m [INFO] ç¼ºå¤±å€¼å¤„ç†ã€å˜é‡å‰”é™¤åŠæ•°æ®æ ¼å¼è½¬æ¢ \033[0m "
echo "===================================================================="

echo '>>>>>>>>>>>>>>>> start:' `date`
BEGIN_TIME=`date +%s`

# main dir
PWD=$(cd $(dirname $0); pwd)
cd $PWD 1> /dev/null 2>&1

TASKNAME=task_wj

# hadoop client
HADOOP_HOME=/usr/lib/hadoop-current
HADOOP_PREFIX=/user/devel/path
HADOOP_INPUT_DIR=${HADOOP_PREFIX}/sample_10000.csv
HADOOP_OUTPUT_DIR=${HADOOP_PREFIX}/output/1015-preprocess/

echo $HADOOP_HOME
echo $HADOOP_INPUT_DIR
echo $HADOOP_OUTPUT_DIR

hadoop fs -rmr $HADOOP_OUTPUT_DIR #åˆ é™¤å·²æœ‰çš„outputæ–‡ä»¶å¤¹

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
    -input ${HADOOP_INPUT_DIR} \
    -output ${HADOOP_OUTPUT_DIR} \
    -file 'clean.py' \
    -mapper "python3 clean.py" 

if [ $? -ne 0 ]; then
    echo 'error'
    exit 1
fi

END_TIME=`date +%s`

echo '******Total cost '  $(($END_TIME-$BEGIN_TIME)) ' seconds'
echo '>>>>>>>>>>>>>>>> end:' `date`
echo "=============SUCCESS=============="

exit 0
```

è¿è¡Œ`sh missing.sh`è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚

#### clean.py

```python
#! /usr/env/python python3

######---------------ç¼ºå¤±å€¼å¤„ç†ã€å˜é‡å‰”é™¤åŠæ•°æ®æ ¼å¼è½¬æ¢---------------#######

import sys,re,csv
import numpy as np
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='logger.log', level=logging.INFO, format=' %(asctime)s - %(levelname)s - %(message)s')
logging.info('Get ready to clean the data...')

# è®¾ç½®å˜é‡åˆå§‹å€¼
isnull = [None,'',' ','-','--']
unit = re.compile(r'(?<=\d) (in|seats|gal|RPM)')

drop_list = [0, 2, 3, 4, 6, 7, 9, 11, 12, 13, 15, 16, 20, 28, 30, 31, 33, 36, 37, 38, 40, 41, 42, 45, 46, 52, 53, 57, 58, 59, 60]
num_list = [1, 8, 10, 14, 21, 22, 25, 26, 27, 34, 35, 39, 43, 44, 48, 50, 51, 63, 64, 65]
cla_list = [5, 17, 18, 19, 23, 24, 29, 32, 49, 54, 56, 61, 62]

# ç†è®ºä¸Šç»è¿‡å¤„ç†åçš„list
drop_list += [47, 55] + [5,23,56,61,62]
num_list  += [66, 67, 68, 69]

# åˆ†ç±»å˜é‡
body_type = ['Minivan','Coupe','Pickup Truck','SUV / Crossover','Hatchback','Wagon','Sedan','Convertible','Van','Others']
fuel_type = ['Biodiesel','Gasoline','Electric','Diesel','Flex Fuel Vehicle','Hybrid','Others']
transmission = ['A','Dual Clutch','CVT','M','Others']
wheel_system = ['4WD','FWD','RWD','4X2','AWD','Others']
listing_color = ['YELLOW','SILVER','RED','WHITE','BLUE','UNKNOWN','BROWN','ORANGE','PURPLE','BLACK','GREEN','TEAL','GRAY','GOLD','Others']
wheel_system_display = ['Four-Wheel Drive','Front-Wheel Drive','4X2','Rear-Wheel Drive','All-Wheel Drive','Others']

def get_dummy(li,lst):
    #é’ˆå¯¹åˆ†ç±»å˜é‡ç”Ÿæˆå“‘å˜é‡
    if li in [None,'',' ','-','--']:
        return [0]*len(lst)
    else:
        dummy_new = [0]*len(lst)
        index = lst.index(li) if li in lst else -1  #å¦‚æœä¸åœ¨å¤‡é€‰åˆ—è¡¨ä¸­,è®°ä¸ºOthers
        dummy_new[index] = 1
        return dummy_new

# å¼€å§‹è¯»å–æ•°æ®
lines = csv.reader(sys.stdin)
for line in lines:
    if line[0] == 'vin': 
        continue
    
    try: 
        line = [i.strip().strip('\n') for i in line]
        assert len(line) == 66  #åˆ¤æ–­æ­£åˆ™è¡¨è¾¾å¼æ˜¯å¦åŒ¹é…æ­£ç¡®

        # å»é™¤å•ä½
        line = [re.sub('--', '', i) for i in line]
        line = [re.sub(unit, '', i) for i in line]

        # é€»è¾‘åˆ†ç±»å˜é‡å¤„ç†
        line = [re.sub('FALSE', '0', i, flags=re.IGNORECASE) for i in line]
        line = [re.sub('TRUE' , '1', i, flags=re.IGNORECASE) for i in line]

        # 47,55 æ–°å¢ä¸€ä¸ªå˜é‡
        power  = re.split(r' hp @ ', re.sub('[",]', '', line[47]))
        torque = re.split(r' lb-ft @ ', re.sub('[",]', '', line[55]))
        line  += power + torque

        # åˆ†ç±»å˜é‡å¤„ç†
        line += get_dummy(line[5],body_type)  # body_type:5 [70-79] 10
        line += get_dummy(line[23],fuel_type)  # fuel_type:23 [80-86] 7
        line += get_dummy(line[56],transmission)  # transmission:56 [87-91] 6
        line += get_dummy(line[61],wheel_system)  # wheel_system:61 [92-97] 6
        line += get_dummy(line[62],wheel_system_display)  # wheel_system_display:62 [98-103] 6

        # å¤„ç†æ•°å€¼å˜é‡
        # line = [float(line[i]) if i in num_list and line[i] not in isnull else line[i] for i in range(len(line))]
        
        # å‰”é™¤å˜é‡
        line = [str(line[i]) for i in range(len(line)) if i not in drop_list]

        if len(line) == 66 :
            print(','.join(line))

    except:
        continue
```

åœ¨ç»è¿‡è¿™ä¸€æ­¥å¤„ç†åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªåˆæ­¥èƒ½å¤Ÿè¿›è¡Œå»ºæ¨¡çš„æ•°æ®é›†ï¼Œæ•°æ®é›†ä¸­çš„æ¯æ¡æ ·æœ¬ç»è¿‡åŸå§‹å¤„ç†åéƒ½åŒ…æ‹¬**66**ä¸ªå˜é‡ï¼Œä¸å†åŒ…å«å­—ç¬¦å‹å˜é‡ï¼Œä¸”å·²ç»å¯¹åˆ†ç±»å˜é‡ç”Ÿæˆäº†å“‘å˜é‡ï¼Œæ•°æ®ç”¨é€—å·åˆ†éš”ã€‚æ•°æ®é›†é¢„è§ˆå¦‚ä¸‹ï¼š

<img src="1-DataClean/img/data-clean.png" alt="image-20201101220326985" style="zoom:50%;" />

